{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17756beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T16:51:56.008040Z",
     "iopub.status.busy": "2024-07-09T16:51:56.007652Z",
     "iopub.status.idle": "2024-07-09T16:51:56.010244Z",
     "shell.execute_reply": "2024-07-09T16:51:56.009950Z"
    },
    "papermill": {
     "duration": 0.016715,
     "end_time": "2024-07-09T16:51:56.010333",
     "exception": false,
     "start_time": "2024-07-09T16:51:55.993618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, max\n",
    "from datetime import date, datetime\n",
    "import subprocess\n",
    "import hashlib\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865e6ac",
   "metadata": {},
   "source": [
    "##### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd5730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate pyspark loggers\n",
    "# pyspark_log = logging.getLogger('pyspark').setLevel(logging.ERROR)\n",
    "# py4j_logger = logging.getLogger(\"py4j\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201d8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First logger for logging the spark job\n",
    "# spark_logger = logging.getLogger('SparkJobs')\n",
    "# spark_logger.setLevel(logging.DEBUG)\n",
    "# today_date = date.today().isoformat()\n",
    "# logger_file_name = f'Logs/SparkLogs/SparkJobsLogs-{today_date}.log'\n",
    "# fh1 = logging.FileHandler(logger_file_name)\n",
    "# fh1.setLevel(logging.DEBUG)\n",
    "# fh1.setFormatter(logging.Formatter('%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S'))\n",
    "# spark_logger.addHandler(fh1)\n",
    "\n",
    "# # Second logger for reporting some information about the files\n",
    "# data_logger = logging.getLogger('DataLogs')\n",
    "# current_time = datetime.now()\n",
    "# data_logger.setLevel(logging.DEBUG)\n",
    "# if not os.path.exists(f'Logs/DataLogs/{today_date}'):\n",
    "#     os.makedirs(f'Logs/DataLogs/{today_date}')\n",
    "# data_logs_file_name = f'Logs/DataLogs/{today_date}/DataInfo-H-{current_time.hour}.log'\n",
    "# fh2 = logging.FileHandler(data_logs_file_name)\n",
    "# fh2.setLevel(logging.DEBUG)\n",
    "# fh2.setFormatter(logging.Formatter('%(message)s'))\n",
    "# data_logger.addHandler(fh2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73563f1",
   "metadata": {},
   "source": [
    "#### Staring the Session and Setting up pyspark Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80811c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .appName(\"DataTransformation\")\\\n",
    "    .config(\"spark.eventLog.logBlockUpdates.enabled\", True)\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/storage_layer/gold/\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "    # .config (\"spark.sql.hive.convertMetastoreOrc\",\"false\")\\\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5779513",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date = date.today().isoformat()\n",
    "current_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f8ddb",
   "metadata": {},
   "source": [
    "#### Some Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e8381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a df, sk and table_id  name and adds the surrogate to the df\n",
    "def add_surrogate_key(df, sk_name, table_id, current_max_sk=0):\n",
    "    df = df.withColumn(sk_name, sf.row_number().over(Window.orderBy(table_id)) + current_max_sk)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77de7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if directory is empty or not\n",
    "def is_directory_empty(dir):\n",
    "    # creates the directory if it don't exist\n",
    "    subprocess.getoutput(f'hdfs dfs -mkdir -p {dir}')\n",
    "    # counts the files in the directory to check if it's empty or not\n",
    "    files_df = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").load(dir)\n",
    "    file_count = files_df.count()\n",
    "    return file_count == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b2c25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hashing function\n",
    "def concat_and_hash(*args):\n",
    "    concatenated_row = ','.join([str(arg) for arg in args])\n",
    "    return hashlib.sha256(str(concatenated_row).encode('utf-8')).hexdigest()\n",
    "    \n",
    "# registering it as a udf to use it later\n",
    "concat_and_hash_udf = sf.udf(concat_and_hash, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5999ce0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsteps:\\n1- define the schema\\n2- read the file\\n3- create the dir if not exists\\n4.0- check if the dir is empty\\n    - if yes:\\n        a. add sk\\n        b. reorder the columns for the sk to be first\\n        4.3- write the final df to the dir\\n    - if no: # that means that there are files in the dir\\n        4.1- read the files in the dir as a df\\n        4.2- get max sk\\n        4.3- drop the sk column\\n        4.4- add row level hash to both dataframes \\n        4.5- hash compare them and get the new data \\n        4.6 - get the max sk from the current data\\n        4.7- add sk to the new data and reorder the columns\\n        4.8- append the new data to the files in gold\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "steps:\n",
    "1- define the schema\n",
    "2- read the file\n",
    "3- create the dir if not exists\n",
    "4.0- check if the dir is empty\n",
    "    - if yes:\n",
    "        a. add sk\n",
    "        b. reorder the columns for the sk to be first\n",
    "        4.3- write the final df to the dir\n",
    "    - if no: # that means that there are files in the dir\n",
    "        4.1- read the files in the dir as a df\n",
    "        4.2- get max sk\n",
    "        4.3- drop the sk column\n",
    "        4.4- add row level hash to both dataframes \n",
    "        4.5- hash compare them and get the new data \n",
    "        4.6 - get the max sk from the current data\n",
    "        4.7- add sk to the new data and reorder the columns\n",
    "        4.8- append the new data to the files in gold\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed43237",
   "metadata": {},
   "source": [
    "### The Transformation Algorithm:\n",
    "\n",
    "1. Define the schema\n",
    "2. Read the file\n",
    "3. Create the directory if not exists\n",
    "4. Check if the directory is empty\n",
    "   - Yes:\n",
    "     1. Add SK\n",
    "     2. Reorder the columns for the SK to be first\n",
    "     3. Write the final DataFrame to the directory\n",
    "   - No:\n",
    "     1. Read the files in the directory as a DataFrame\n",
    "     2. Get max SK\n",
    "     3. Drop the SK column\n",
    "     4. Add row level hash to both DataFrames\n",
    "     5. Compare hashes to get new data\n",
    "     6. Get the max SK from the current data\n",
    "     7. Add SK to the new data and reorder the columns\n",
    "     8. Append the new data to the files in 'gold'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a459b",
   "metadata": {},
   "source": [
    "## The Actual Transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f70e9",
   "metadata": {},
   "source": [
    "### branch File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec630fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Environment Variables:\n",
    "db=\"qcompany\"\n",
    "table = \"branch\"\n",
    "table_name = \"branch_dim\"\n",
    "schema = f\"{table}_schema\"\n",
    "table_key = f\"{table}_key\"\n",
    "table_id = f\"{table}_id\"\n",
    "table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/{table_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "500fc305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"branch_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"branch_location\", StringType(), nullable=False),\n",
    "    StructField(\"branch_establish_date\", DateType(), nullable=False),\n",
    "    StructField(\"branch_class\", StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e1c78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- read the file\n",
    "new_data_path = '/user/itversity/q-retail-company/silver/2024-07-10/hour-17/branches_cleaned.csv'\n",
    "df2_path = '/user/itversity/q-retail-company/silver/2024-07-10/hour-18/branches_cleaned.csv'\n",
    "\n",
    "new_data_df = spark.read.csv(new_data_path, header=True, schema=schema)\n",
    "df2 = spark.read.csv(df2_path, header=True, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b2de3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty dir, let's fill it!\n",
      "----------------------------------------------------------------------------------\n",
      "adding the surrogate key\n",
      "----------------------------------------------------------------------------------\n",
      "preparing the data to be written\n",
      "----------------------------------------------------------------------------------\n",
      "writing the df as a parquet to hdfs\n",
      "----------------------------------------------------------------------------------\n",
      "\n",
      " Done\n"
     ]
    }
   ],
   "source": [
    "if is_directory_empty(table_dir):\n",
    "    print(\"Empty dir, let's fill it!\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"adding the surrogate key\")\n",
    "    df1sk = add_surrogate_key(new_data_df, table_key, table_id)\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"preparing the data to be written\")\n",
    "    df1sk = df1sk.select(f\"{table_key}\", *[col for col in df1sk.columns if col != f\"{table_key}\"])\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing the df as a parquet to hdfs\")\n",
    "    df1sk.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Done\")\n",
    "else:\n",
    "    print(\"Dir is not empty let's hash compare them!\")\n",
    "    current_df_sk = spark.read.parquet(table_dir, header=True, inferSchema=True)\n",
    "    print(f\"reading the data in the current {table_name} data\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"getting the max surrogate key to use later on when appening the new data... \")\n",
    "    max_sk = current_df_sk.agg(sf.max(table_key)).collect()[0][0]\n",
    "    print(f\"current max {table_key} is {max_sk}, the next {table_key} will be {max_sk+1}\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    current_df = current_df_sk.drop(table_key)\n",
    "    new_data_df = new_data_df.drop(table_key)\n",
    "    new_data_df = new_data_df.drop(\"hash\") # in case we re ran the same code twice\n",
    "    print(f\"hashing current & new data to compare them\")\n",
    "    current_df = current_df.withColumn(\"hash\", concat_and_hash_udf(*[sf.col(column).cast(\"string\") for column in current_df.columns]))\n",
    "    new_data_df = new_data_df.withColumn(\"hash\", concat_and_hash_udf(*[sf.col(column).cast(\"string\") for column in new_data_df.columns]))\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    new_rows_df = new_data_df.join(current_df, new_data_df.hash == current_df.hash, \"left_anti\")\n",
    "    # another medthod \n",
    "    # new_rows_df = new_data_df.exceptAll(current_df).show()\n",
    "    print(\"comparing hashes and removing the duplicates\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"preparing the net new data to append to the current data \")\n",
    "    new_rows_df = new_rows_df.drop(\"hash\")\n",
    "    new_rows_df = add_surrogate_key(new_rows_df, table_key, table_id, max_sk)\n",
    "    new_rows_df = new_rows_df.select(f\"{table_key}\", *[col for col in new_rows_df.columns if col != f\"{table_key}\"])\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing (Appending) the new data to HDFS\")\n",
    "    new_rows_df.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Mission Passed... Respect+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b63d7ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+---------------------+------------+\n",
      "|branch_key|branch_id|branch_location|branch_establish_date|branch_class|\n",
      "+----------+---------+---------------+---------------------+------------+\n",
      "|         1|        1|       New York|           2017-01-15|           A|\n",
      "|         2|        2|    Los Angeles|           2016-07-28|           B|\n",
      "|         3|        3|        Chicago|           2015-03-10|           A|\n",
      "|         4|        4|        Houston|           2016-11-05|           D|\n",
      "|         5|        5|        Phoenix|           2017-09-20|           C|\n",
      "+----------+---------+---------------+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for testing purposes\n",
    "spark.sql(\"REFRESH TABLES\")\n",
    "spark.read.parquet(table_dir, header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb3bc0",
   "metadata": {},
   "source": [
    "### agent File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e8acdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Environment Variables:\n",
    "db=\"qcompany\"\n",
    "table = \"agent\"\n",
    "table_name = \"agent_dim\"\n",
    "schema = f\"{table}_schema\"\n",
    "table_key = f\"{table}_key\"\n",
    "table_id = f\"{table}_id\"\n",
    "table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/{table_name}\"\n",
    "\n",
    "# del(table_key,table_id, dir )\n",
    "\n",
    "# dir = f\"/user/itversity/q-retail-company/gold/{db}.db/{table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ca04aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"agent_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"agent_name\", StringType(), nullable=False),\n",
    "    StructField(\"agent_hire_date\", DateType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6466b8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+---------------+\n",
      "|agent_id|        agent_name|agent_hire_date|\n",
      "+--------+------------------+---------------+\n",
      "|       1|          John Doe|     2020-06-03|\n",
      "|       2|        Jane Smith|     2018-05-13|\n",
      "|       3|   Michael Johnson|     2021-10-03|\n",
      "|       4|       Emily Brown|     2020-10-25|\n",
      "|       5|      David Wilson|     2021-04-08|\n",
      "|       6|       Emma Taylor|     2019-03-28|\n",
      "|       7|Christopher Miller|     2020-01-11|\n",
      "|       8|      Olivia Davis|     2021-10-24|\n",
      "|       9|   Daniel Martinez|     2018-10-08|\n",
      "|      10|      Sophia Moore|     2019-05-25|\n",
      "+--------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2- read the file\n",
    "new_data_path = '/user/itversity/q-retail-company/silver/2024-07-10/hour-17/agent_cleaned.csv'\n",
    "df2_path = '/user/itversity/q-retail-company/silver/2024-07-10/hour-18/agent_cleaned.csv'\n",
    "\n",
    "new_data_df = spark.read.csv(new_data_path, header=True, schema=schema)\n",
    "df2 = spark.read.csv(df2_path, header=True, schema=schema)\n",
    "\n",
    "new_data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec7b8316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty dir, let's fill it!\n",
      "----------------------------------------------------------------------------------\n",
      "adding the surrogate key\n",
      "----------------------------------------------------------------------------------\n",
      "preparing the data to be written\n",
      "----------------------------------------------------------------------------------\n",
      "writing the df as a parquet to hdfs\n",
      "----------------------------------------------------------------------------------\n",
      "\n",
      " Done\n"
     ]
    }
   ],
   "source": [
    "if is_directory_empty(table_dir):\n",
    "    print(\"Empty dir, let's fill it!\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"adding the surrogate key\")\n",
    "    df1sk = add_surrogate_key(new_data_df, table_key, table_id)\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"preparing the data to be written\")\n",
    "    df1sk = df1sk.select(f\"{table_key}\", *[col for col in df1sk.columns if col != f\"{table_key}\"])\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing the df as a parquet to hdfs\")\n",
    "    df1sk.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Done\")\n",
    "else:\n",
    "    print(\"Dir is not empty let's hash compare them!\")\n",
    "    current_df_sk = spark.read.parquet(table_dir, header=True, inferSchema=True)\n",
    "    print(f\"reading the data in the current {table_name} data\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"getting the max surrogate key to use later on when appening the new data... \")\n",
    "    max_sk = current_df_sk.agg(sf.max(table_key)).collect()[0][0]\n",
    "    print(f\"current max {table_key} is {max_sk}, the next {table_key} will be {max_sk+1}\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    current_df = current_df_sk.drop(table_key)\n",
    "    new_data_df = new_data_df.drop(table_key)\n",
    "    new_data_df = new_data_df.drop(\"hash\") # in case we re ran the same code twice\n",
    "    print(f\"hashing current & new data to compare them\")\n",
    "    current_df = current_df.withColumn(\"hash\", concat_and_hash_udf(*[sf.col(column).cast(\"string\") for column in current_df.columns]))\n",
    "    new_data_df = new_data_df.withColumn(\"hash\", concat_and_hash_udf(*[sf.col(column).cast(\"string\") for column in new_data_df.columns]))\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    new_rows_df = new_data_df.join(current_df, new_data_df.hash == current_df.hash, \"left_anti\")\n",
    "    # another medthod \n",
    "    # new_rows_df = new_data_df.exceptAll(current_df).show()\n",
    "    print(\"comparing hashes and removing the duplicates\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"preparing the net new data to append to the current data \")\n",
    "    new_rows_df = new_rows_df.drop(\"hash\")\n",
    "    new_rows_df = add_surrogate_key(new_rows_df, table_key, table_id, max_sk)\n",
    "    new_rows_df = new_rows_df.select(f\"{table_key}\", *[col for col in new_rows_df.columns if col != f\"{table_key}\"])\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing (Appending) the new data to HDFS\")\n",
    "    new_rows_df.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Mission Passed... Respect+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4405a337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+---------------+\n",
      "|agent_key|agent_id|        agent_name|agent_hire_date|\n",
      "+---------+--------+------------------+---------------+\n",
      "|        1|       1|          John Doe|     2020-06-03|\n",
      "|        2|       2|        Jane Smith|     2018-05-13|\n",
      "|        3|       3|   Michael Johnson|     2021-10-03|\n",
      "|        4|       4|       Emily Brown|     2020-10-25|\n",
      "|        5|       5|      David Wilson|     2021-04-08|\n",
      "|        6|       6|       Emma Taylor|     2019-03-28|\n",
      "|        7|       7|Christopher Miller|     2020-01-11|\n",
      "|        8|       8|      Olivia Davis|     2021-10-24|\n",
      "|        9|       9|   Daniel Martinez|     2018-10-08|\n",
      "|       10|      10|      Sophia Moore|     2019-05-25|\n",
      "+---------+--------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for testing purposes\n",
    "spark.sql(\"REFRESH TABLES\")\n",
    "spark.read.parquet(table_dir, header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f425f798",
   "metadata": {},
   "source": [
    "### transactions File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "024a9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the expected schema\n",
    "main_transactions_schema = StructType([\n",
    "    StructField(\"transaction_date\", DateType(), nullable=False),\n",
    "    StructField(\"transaction_id\", StringType(), nullable=False),\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"customer_fname\", StringType(), nullable=False),\n",
    "    StructField(\"customer_lname\", StringType(), nullable=False),\n",
    "    StructField(\"customer_email\", StringType(), nullable=False),\n",
    "    StructField(\"agent_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"branch_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"product_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"product_name\", StringType(), nullable=False),\n",
    "    StructField(\"product_category\", StringType(), nullable=False),\n",
    "    StructField(\"offer_1\", BooleanType(), nullable=True),\n",
    "    StructField(\"offer_2\", BooleanType(), nullable=True),\n",
    "    StructField(\"offer_3\", BooleanType(), nullable=True),\n",
    "    StructField(\"offer_4\", BooleanType(), nullable=True),\n",
    "    StructField(\"offer_5\", BooleanType(), nullable=True),\n",
    "    StructField(\"units\", IntegerType(), nullable=False),\n",
    "    StructField(\"unit_price\", FloatType(), nullable=False),\n",
    "    StructField(\"is_online\", StringType(), nullable=False),\n",
    "    StructField(\"payment_method\", StringType(), nullable=False),\n",
    "    StructField(\"shipping_address\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d02720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- read the file\n",
    "new_data_path = '/user/itversity/q-retail-company/silver/2024-07-10/hour-17/transaction_cleaned.csv'\n",
    "df2_path = '/user/itversity/q-retail-company/silver/2024-07-10/hour-18/transaction_cleaned.csv'\n",
    "\n",
    "new_main_data_df = spark.read.csv(new_data_path, header=True, schema=main_transactions_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9a21f8",
   "metadata": {},
   "source": [
    "#### customer dim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0960cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Environment Variables:\n",
    "db=\"qcompany\"\n",
    "table = \"customer\"\n",
    "table_name = \"customer_dim\"\n",
    "schema = f\"{table}_schema\"\n",
    "table_key = f\"{table}_key\"\n",
    "table_id = f\"{table}_id\"\n",
    "table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/{table_name}\"\n",
    "\n",
    "# del(table_key,table_id, dir )\n",
    "\n",
    "# dir = f\"/user/itversity/q-retail-company/gold/{db}.db/{table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db0c4926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- define the schema\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"customer_fname\", StringType(), nullable=False),\n",
    "    StructField(\"customer_lname\", StringType(), nullable=False),\n",
    "    StructField(\"customer_email\", StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc4a2927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------------+\n",
      "|customer_id|customer_fname|customer_lname|      customer_email|\n",
      "+-----------+--------------+--------------+--------------------+\n",
      "|      85526|        Sophia|         Smith|sophia.smith@hotm...|\n",
      "|      85524|     Alexander|        Miller|alexander.miller@...|\n",
      "|      85502|       William|         Jones|william.jones@gma...|\n",
      "|      85481|     Alexander|      Williams|alexander.william...|\n",
      "|      85558|         James|         Smith|james.smith@outlo...|\n",
      "|      85517|        Sophia|         Davis|sophia.davis@hotm...|\n",
      "|      85555|          John|       Johnson|john.johnson@hotm...|\n",
      "|      85541|        Olivia|        Wilson|olivia.wilson@out...|\n",
      "|      85545|          John|        Wilson|john.wilson@gmail...|\n",
      "|      85494|       William|         Davis|william.davis@gma...|\n",
      "|      85488|        Olivia|        Wilson|olivia.wilson@yah...|\n",
      "|      85554|       William|        Wilson|william.wilson@ya...|\n",
      "|      85508|           Ava|         Brown| ava.brown@gmail.com|\n",
      "|      85539|          Emma|         Davis|emma.davis@hotmai...|\n",
      "|      85486|        Olivia|         Smith|olivia.smith@outl...|\n",
      "|      85525|        Olivia|        Miller|olivia.miller@gma...|\n",
      "|      85540|         James|       Johnson|james.johnson@gma...|\n",
      "|      85544|     Alexander|        Miller|alexander.miller@...|\n",
      "|      85518|           Ava|        Taylor|ava.taylor@gmail.com|\n",
      "|      85516|       William|       Johnson|william.johnson@g...|\n",
      "+-----------+--------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2- get the new customer df (subset from the new transaction file)\n",
    "\n",
    "\n",
    "new_data_df = new_main_data_df.select(\"customer_id\", \"customer_fname\", \"customer_lname\", \"customer_email\").distinct()\n",
    "new_data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8945872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty dir, let's fill it!\n",
      "----------------------------------------------------------------------------------\n",
      "adding the surrogate key\n",
      "----------------------------------------------------------------------------------\n",
      "preparing the data to be written\n",
      "----------------------------------------------------------------------------------\n",
      "writing the df as a parquet to hdfs\n",
      "----------------------------------------------------------------------------------\n",
      "\n",
      " Done\n"
     ]
    }
   ],
   "source": [
    "if is_directory_empty(table_dir):\n",
    "    print(\"Empty dir, let's fill it!\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"adding the surrogate key\")\n",
    "    df1sk = add_surrogate_key(new_data_df, table_key, table_id)\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"preparing the data to be written\")\n",
    "    df1sk = df1sk.select(f\"{table_key}\", *[col for col in df1sk.columns if col != f\"{table_key}\"])\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing the df as a parquet to hdfs\")\n",
    "    df1sk.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Done\")\n",
    "else:\n",
    "    print(\"Dir is not empty let's hash compare them!\")\n",
    "    current_df_sk = spark.read.parquet(table_dir, header=True, inferSchema=True)\n",
    "    print(f\"reading the data in the current {table_name} data\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"getting the max surrogate key to use later on when appening the new data... \")\n",
    "    max_sk = current_df_sk.agg(sf.max(table_key)).collect()[0][0]\n",
    "    print(f\"current max {table_key} is {max_sk}, the next {table_key} will be {max_sk+1}\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    current_df = current_df_sk.drop(table_key)\n",
    "    new_data_df = new_data_df.drop(table_key)\n",
    "    new_data_df = new_data_df.drop(\"hash\") # in case we re ran the same code twice\n",
    "    print(f\"hashing current & new data to compare them\")\n",
    "    current_df = current_df.withColumn(\"hash\", concat_and_hash_udf(*[sf.col(column).cast(\"string\") for column in current_df.columns]))\n",
    "    new_data_df = new_data_df.withColumn(\"hash\", concat_and_hash_udf(*[sf.col(column).cast(\"string\") for column in new_data_df.columns]))\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    new_rows_df = new_data_df.join(current_df, new_data_df.hash == current_df.hash, \"left_anti\")\n",
    "    # another medthod \n",
    "    # new_rows_df = new_data_df.exceptAll(current_df).show()\n",
    "    print(\"comparing hashes and removing the duplicates\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"preparing the net new data to append to the current data \")\n",
    "    new_rows_df = new_rows_df.drop(\"hash\")\n",
    "    new_rows_df = add_surrogate_key(new_rows_df, table_key, table_id, max_sk)\n",
    "    new_rows_df = new_rows_df.select(f\"{table_key}\", *[col for col in new_rows_df.columns if col != f\"{table_key}\"])\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing (Appending) the new data to HDFS\")\n",
    "    new_rows_df.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Mission Passed... Respect+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cc8075e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "+------------+-----------+--------------+--------------+--------------------+\n",
      "|customer_key|customer_id|customer_fname|customer_lname|      customer_email|\n",
      "+------------+-----------+--------------+--------------+--------------------+\n",
      "|           1|      85462|          John|         Smith|john.smith@yahoo.com|\n",
      "|           2|      85463|        Olivia|         Smith|olivia.smith@outl...|\n",
      "|           3|      85464|       Michael|        Miller|michael.miller@ou...|\n",
      "|           4|      85465|        Sophia|        Miller|sophia.miller@out...|\n",
      "|           5|      85466|          Emma|         Brown|emma.brown@yahoo.com|\n",
      "|           6|      85467|          Emma|        Miller|emma.miller@yahoo...|\n",
      "|           7|      85468|          Emma|        Wilson|emma.wilson@outlo...|\n",
      "|           8|      85469|       Michael|      Williams|michael.williams@...|\n",
      "|           9|      85470|          John|        Taylor|john.taylor@yahoo...|\n",
      "|          10|      85471|     Alexander|        Miller|alexander.miller@...|\n",
      "|          11|      85472|        Olivia|        Miller|olivia.miller@yah...|\n",
      "|          12|      85473|           Mia|      Williams|mia.williams@yaho...|\n",
      "|          13|      85474|          Emma|        Wilson|emma.wilson@gmail...|\n",
      "|          14|      85475|       William|        Taylor|william.taylor@gm...|\n",
      "|          15|      85476|          Emma|        Taylor|emma.taylor@outlo...|\n",
      "|          16|      85477|        Sophia|         Jones|sophia.jones@gmai...|\n",
      "|          17|      85478|        Sophia|         Brown|sophia.brown@gmai...|\n",
      "|          18|      85479|       Michael|         Jones|michael.jones@gma...|\n",
      "|          19|      85480|        Olivia|         Davis|olivia.davis@hotm...|\n",
      "|          20|      85481|     Alexander|      Williams|alexander.william...|\n",
      "+------------+-----------+--------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for testing purposes\n",
    "spark.sql(\"REFRESH TABLES\")\n",
    "test_df = spark.read.parquet(table_dir, header=True, inferSchema=True)\n",
    "print(test_df.count())\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2cb3a",
   "metadata": {},
   "source": [
    "#### product dim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "345d63fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/itversity/q-retail-company/gold/qcompany.db/product_dim\n"
     ]
    }
   ],
   "source": [
    "# Required Environment Variables:\n",
    "db=\"qcompany\"\n",
    "table = \"product\"\n",
    "table_name = \"product_dim\"\n",
    "schema = f\"{table}_schema\"\n",
    "table_key = f\"{table}_key\"\n",
    "table_id = f\"{table}_id\"\n",
    "table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/{table_name}\"\n",
    "\n",
    "print(table_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ff702c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- define the schema\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"product_name\", StringType(), nullable=False),\n",
    "    StructField(\"product_price\", DoubleType(), nullable=False),\n",
    "    StructField(\"product_category\", StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a660ce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+----------------+\n",
      "|product_id|product_name|unit_price|product_category|\n",
      "+----------+------------+----------+----------------+\n",
      "|         1|      Laptop|    999.99|     Electronics|\n",
      "|         2|  Smartphone|    699.99|     Electronics|\n",
      "|         3|      Tablet|    299.99|     Electronics|\n",
      "|         4|  Headphones|     99.99|     Electronics|\n",
      "|         5|     T-Shirt|     19.99|        Clothing|\n",
      "|         6|       Jeans|     49.99|        Clothing|\n",
      "|         7|       Dress|     59.99|        Clothing|\n",
      "|         8|    Sneakers|     79.99|        Footwear|\n",
      "|         9|       Boots|    129.99|        Footwear|\n",
      "|        10|     Sandals|     39.99|        Footwear|\n",
      "|        11|          TV|    899.99|     Electronics|\n",
      "|        12|     Monitor|    299.99|     Electronics|\n",
      "|        13|     Printer|    149.99|     Electronics|\n",
      "|        14|      Camera|    399.99|     Electronics|\n",
      "|        15|      Hoodie|     29.99|        Clothing|\n",
      "|        16|       Skirt|     39.99|        Clothing|\n",
      "|        17|      Blouse|     29.99|        Clothing|\n",
      "|        18|       Boots|    149.99|        Footwear|\n",
      "|        19|     Sandals|     29.99|        Footwear|\n",
      "|        20|       Heels|     59.99|        Footwear|\n",
      "+----------+------------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2- get the new customer df (subset from the new transaction file)\n",
    "\n",
    "\n",
    "new_data_df = new_main_data_df.select(\"product_id\", \"product_name\", \"unit_price\", \"product_category\").distinct().orderBy(\"product_id\")\n",
    "\n",
    "new_data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "655f80ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty dir, let's fill it!\n",
      "----------------------------------------------------------------------------------\n",
      "adding the surrogate key\n",
      "----------------------------------------------------------------------------------\n",
      "preparing the data to be written\n",
      "----------------------------------------------------------------------------------\n",
      "writing the df as a parquet to hdfs\n",
      "----------------------------------------------------------------------------------\n",
      "\n",
      " Done\n"
     ]
    }
   ],
   "source": [
    "if is_directory_empty(table_dir):\n",
    "    print(\"Empty dir, let's fill it!\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"adding the surrogate key\")\n",
    "    df1sk = add_surrogate_key(new_data_df, table_key, table_id)\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"preparing the data to be written\")\n",
    "    df1sk = df1sk.select(f\"{table_key}\", *[col for col in df1sk.columns if col != f\"{table_key}\"])\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing the df as a parquet to hdfs\")\n",
    "    df1sk.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Done\")\n",
    "else:\n",
    "    print(\"Dir is not empty let's hash compare them!\")\n",
    "    current_df_sk = spark.read.parquet(table_dir, header=True, inferSchema=True)\n",
    "    print(f\"reading the data in the current {table_name} data\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"getting the max surrogate key to use later on when appening the new data... \")\n",
    "    max_sk = current_df_sk.agg(sf.max(table_key)).collect()[0][0]\n",
    "    print(f\"current max {table_key} is {max_sk}, the next {table_key} will be {max_sk+1}\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    current_df = current_df_sk.drop(table_key)\n",
    "    new_data_df = new_data_df.drop(table_key)\n",
    "    new_data_df = new_data_df.drop(\"hash\") # in case we re ran the same code twice\n",
    "    print(f\"hashing current & new data to compare them\")\n",
    "    current_df = current_df.withColumn(\"hash\", concat_and_hash_udf(*[sf.col(column).cast(\"string\") for column in current_df.columns]))\n",
    "    new_data_df = new_data_df.withColumn(\"hash\", concat_and_hash_udf(*[sf.col(column).cast(\"string\") for column in new_data_df.columns]))\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    new_rows_df = new_data_df.join(current_df, new_data_df.hash == current_df.hash, \"left_anti\")\n",
    "    # another medthod \n",
    "    # new_rows_df = new_data_df.exceptAll(current_df).show()\n",
    "    print(\"comparing hashes and removing the duplicates\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"preparing the net new data to append to the current data \")\n",
    "    new_rows_df = new_rows_df.drop(\"hash\")\n",
    "    new_rows_df = add_surrogate_key(new_rows_df, table_key, table_id, max_sk)\n",
    "    new_rows_df = new_rows_df.select(f\"{table_key}\", *[col for col in new_rows_df.columns if col != f\"{table_key}\"])\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing (Appending) the new data to HDFS\")\n",
    "    new_rows_df.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Mission Passed... Respect+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "482eb8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "+-----------+----------+------------+----------+----------------+\n",
      "|product_key|product_id|product_name|unit_price|product_category|\n",
      "+-----------+----------+------------+----------+----------------+\n",
      "|          1|         1|      Laptop|    999.99|     Electronics|\n",
      "|          2|         2|  Smartphone|    699.99|     Electronics|\n",
      "|          3|         3|      Tablet|    299.99|     Electronics|\n",
      "|          4|         4|  Headphones|     99.99|     Electronics|\n",
      "|          5|         5|     T-Shirt|     19.99|        Clothing|\n",
      "|          6|         6|       Jeans|     49.99|        Clothing|\n",
      "|          7|         7|       Dress|     59.99|        Clothing|\n",
      "|          8|         8|    Sneakers|     79.99|        Footwear|\n",
      "|          9|         9|       Boots|    129.99|        Footwear|\n",
      "|         10|        10|     Sandals|     39.99|        Footwear|\n",
      "|         11|        11|          TV|    899.99|     Electronics|\n",
      "|         12|        12|     Monitor|    299.99|     Electronics|\n",
      "|         13|        13|     Printer|    149.99|     Electronics|\n",
      "|         14|        14|      Camera|    399.99|     Electronics|\n",
      "|         15|        15|      Hoodie|     29.99|        Clothing|\n",
      "|         16|        16|       Skirt|     39.99|        Clothing|\n",
      "|         17|        17|      Blouse|     29.99|        Clothing|\n",
      "|         18|        18|       Boots|    149.99|        Footwear|\n",
      "|         19|        19|     Sandals|     29.99|        Footwear|\n",
      "|         20|        20|       Heels|     59.99|        Footwear|\n",
      "+-----------+----------+------------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for testing purposes\n",
    "spark.sql(\"REFRESH TABLES\")\n",
    "test_df = spark.read.parquet(table_dir, header=True, inferSchema=True)\n",
    "print(test_df.count())\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431d24d",
   "metadata": {},
   "source": [
    "### Transaction Facts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcd180",
   "metadata": {},
   "source": [
    "##### Feature Engineering \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d2d1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the transactions data\n",
    "main_transactions = new_main_data_df.drop(\"customer_fname\", \"customer_lname\", \"customer_email\", \"product_name\", \"product_price\", \"product_category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d928f",
   "metadata": {},
   "source": [
    "Offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e908f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the redeemed offer column\n",
    "main_transactions = main_transactions.withColumn(\"offer_redeemed\", \n",
    "                                                 sf.when(sf.col(\"offer_1\") == True, \"offer_1\")\\\n",
    "                                                .when(sf.col(\"offer_2\") == True, \"offer_2\")\\\n",
    "                                                .when(sf.col(\"offer_3\") == True, \"offer_3\")\\\n",
    "                                                .when(sf.col(\"offer_4\") == True, \"offer_4\")\\\n",
    "                                                .when(sf.col(\"offer_5\") == True, \"offer_5\")\\\n",
    "                                                .otherwise(\"NA\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8be4ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adding the offer percentage column\n",
    "main_transactions = main_transactions.withColumn(\"discount_pct\", \n",
    "                                                 sf.when(sf.col(\"offer_redeemed\") == \"offer_1\", 5)\\\n",
    "                                                 .when(sf.col(\"offer_redeemed\") == \"offer_2\", 10)\\\n",
    "                                                 .when(sf.col(\"offer_redeemed\") == \"offer_3\", 15)\\\n",
    "                                                 .when(sf.col(\"offer_redeemed\") == \"offer_4\", 20)\\\n",
    "                                                 .when(sf.col(\"offer_redeemed\") == \"offer_5\", 25)\\\n",
    "                                                .otherwise(0)\n",
    ")\n",
    "\n",
    "# main_transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61f652a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the offers(1..5) columns\n",
    "main_transactions = main_transactions.drop(\"offer_1\", \"offer_2\", \"offer_3\", \"offer_4\", \"offer_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6593210",
   "metadata": {},
   "source": [
    "Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bba41945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the total price column\n",
    "main_transactions = main_transactions.withColumn(\"total_price\", sf.col(\"unit_price\") * sf.col(\"units\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "585107aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the final price column\n",
    "main_transactions = main_transactions.withColumn(\"final_price\", sf.round(sf.col(\"total_price\") * (1 - sf.col(\"discount_pct\") / 100), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a13976",
   "metadata": {},
   "source": [
    "Now, Separating the Transactions to online and offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2a31acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_transactions = main_transactions.filter(sf.col(\"is_online\") == \"yes\")\n",
    "offline_transactions = main_transactions.filter(sf.col(\"is_online\") == \"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc44797",
   "metadata": {},
   "source": [
    "#### Online Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c984a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_transactions = online_transactions.drop(\"branch_id\", \"agent_id\")\n",
    "# online_transactions.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ca590",
   "metadata": {},
   "source": [
    "Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd287ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # online_transactions = \n",
    "online_transactions = online_transactions.withColumn(\"address\", sf.split(online_transactions[\"shipping_address\"], \"/\").getItem(0))\n",
    "online_transactions = online_transactions.withColumn(\"shipping_city\", sf.split(online_transactions[\"shipping_address\"], \"/\").getItem(1))\n",
    "online_transactions = online_transactions.withColumn(\"shipping_state\", sf.split(online_transactions[\"shipping_address\"], \"/\").getItem(2))\n",
    "online_transactions = online_transactions.withColumn(\"shipping_postal_code\", sf.split(online_transactions[\"shipping_address\"], \"/\").getItem(3))\n",
    "online_transactions = online_transactions.drop(\"shipping_address\")\n",
    "# online_transactions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "890d8396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----------+----------+-----+----------+---------+--------------+--------------+------------+-----------+-----------+------------------+-------------+--------------+--------------------+\n",
      "|transaction_date|  transaction_id|customer_id|product_id|units|unit_price|is_online|payment_method|offer_redeemed|discount_pct|total_price|final_price|           address|shipping_city|shipping_state|shipping_postal_code|\n",
      "+----------------+----------------+-----------+----------+-----+----------+---------+--------------+--------------+------------+-----------+-----------+------------------+-------------+--------------+--------------------+\n",
      "|      2022-05-11|trx-254567854494|      85492|        27|   10|     29.99|      yes|        PayPal|            NA|           0|      299.9|      299.9|7002 Secrest Court|       Arvada|            CO|               80007|\n",
      "|      2022-04-06|trx-585776417704|      85521|        28|    5|     19.99|      yes|        Stripe|            NA|           0|      99.95|      99.95|    175 Creek Road|    Castleton|            VT|               05735|\n",
      "+----------------+----------------+-----------+----------+-----+----------+---------+--------------+--------------+------------+-----------+-----------+------------------+-------------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_transactions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a41f13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to join  the customer_dim, product_dim to the fact to get their keys\n",
    "customer_table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/customer_dim\"\n",
    "product_table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/product_dim\"\n",
    "\n",
    "current_customer_dim = spark.read.parquet(customer_table_dir, header=True, inferSchema=True)\n",
    "current_product_dim = spark.read.parquet(product_table_dir, header=True, inferSchema=True)\n",
    "\n",
    "online_transactions_with_c_key = online_transactions.join(current_customer_dim.select(\"customer_id\", \"customer_key\"), online_transactions.customer_id == current_customer_dim.customer_id, \"left\")\n",
    "online_transactions_with_c_p_key = online_transactions_with_c_key.join(current_product_dim.select(\"product_id\", \"product_key\"), online_transactions_with_c_key.product_id == current_product_dim.product_id, \"left\")\n",
    "\n",
    "columns_to_exclude = [\"customer_id\", \"product_id\", \"is_online\"]\n",
    "selected_cols = [col for col in online_transactions_with_c_p_key.columns if col not in columns_to_exclude]\n",
    "\n",
    "final_new_online_transactions = online_transactions_with_c_p_key.select(selected_cols)\\\n",
    "                                                            .withColumnRenamed(\"transaction_id\", \"online_transaction_id\")\\\n",
    "                                                            .withColumnRenamed(\"units\", \"quantity\")\\\n",
    "                                                            .withColumnRenamed(\"address\", \"shipping_address\")\n",
    "# final_new_online_transactions.show(2)\n",
    "\n",
    "desired_columns_ordered = ['online_transaction_id', 'transaction_date', 'customer_key', 'product_key', 'unit_price', 'quantity', 'total_price', 'offer_redeemed', 'discount_pct', 'final_price', 'payment_method', 'shipping_address', 'shipping_city', 'shipping_state', 'shipping_postal_code']\n",
    "\n",
    "final_new_online_transactions = final_new_online_transactions.select(desired_columns_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c17659e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/itversity/q-retail-company/gold/qcompany.db/online_transaction_fact\n"
     ]
    }
   ],
   "source": [
    "# Required Environment Variables:\n",
    "db=\"qcompany\"\n",
    "table = \"online_transaction\"\n",
    "table_name = \"online_transaction_fact\"\n",
    "table_key = f\"{table}_key\"\n",
    "table_id = f\"{table}_id\"\n",
    "table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/{table_name}\"\n",
    "\n",
    "print(table_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20bb925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- online_transaction_id: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- customer_key: integer (nullable = true)\n",
      " |-- product_key: integer (nullable = true)\n",
      " |-- unit_price: float (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- total_price: float (nullable = true)\n",
      " |-- offer_redeemed: string (nullable = false)\n",
      " |-- discount_pct: integer (nullable = false)\n",
      " |-- final_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- shipping_address: string (nullable = true)\n",
      " |-- shipping_city: string (nullable = true)\n",
      " |-- shipping_state: string (nullable = true)\n",
      " |-- shipping_postal_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = final_new_online_transactions.schema\n",
    "final_new_online_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48fe8b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty dir, let's fill it!\n",
      "----------------------------------------------------------------------------------\n",
      "adding the surrogate key\n",
      "----------------------------------------------------------------------------------\n",
      "preparing the data to be written\n",
      "----------------------------------------------------------------------------------\n",
      "writing the df as a parquet to hdfs\n",
      "----------------------------------------------------------------------------------\n",
      "\n",
      " Done\n"
     ]
    }
   ],
   "source": [
    "if is_directory_empty(table_dir):\n",
    "    print(\"Empty dir, let's fill it!\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"adding the surrogate key\")\n",
    "    df1sk = add_surrogate_key(final_new_online_transactions, table_key, table_id)\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"preparing the data to be written\")\n",
    "    df1sk = df1sk.select(f\"{table_key}\", *[col for col in df1sk.columns if col != f\"{table_key}\"])\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing the df as a parquet to hdfs\")\n",
    "    df1sk.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Done\")\n",
    "else:\n",
    "    print(\"Dir is not empty let's Append the new transactions!\")\n",
    "    current_df_sk = spark.read.parquet(table_dir, header=True, inferSchema=True)\n",
    "    # current_df_sk.show(1)\n",
    "    print(f\"reading the data in the current {table_name} data\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"getting the max surrogate key to use later on when appening the new data... \")\n",
    "    max_sk = current_df_sk.agg(sf.max(table_key)).collect()[0][0]\n",
    "    print(f\"current max {table_key} is {max_sk}, the next {table_key} will be {max_sk+1}\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing (Appending) the new data to HDFS\")\n",
    "    final_new_online_transactions = add_surrogate_key(final_new_online_transactions, table_key, table_id, max_sk)\n",
    "    final_new_online_transactions = final_new_online_transactions.select(f\"{table_key}\", *[col for col in final_new_online_transactions.columns if col != f\"{table_key}\"])\n",
    "    final_new_online_transactions.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Mission Passed... Respect+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "324c3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------+----------------+------------+-----------+----------+--------+-----------+--------------+------------+-----------+--------------+--------------------+-------------+--------------+--------------------+\n",
      "|online_transaction_key|online_transaction_id|transaction_date|customer_key|product_key|unit_price|quantity|total_price|offer_redeemed|discount_pct|final_price|payment_method|    shipping_address|shipping_city|shipping_state|shipping_postal_code|\n",
      "+----------------------+---------------------+----------------+------------+-----------+----------+--------+-----------+--------------+------------+-----------+--------------+--------------------+-------------+--------------+--------------------+\n",
      "|                     1|     trx-003395056165|      2022-04-20|           4|         25|    499.99|       9|    4499.91|            NA|           0|    4499.91|        Stripe|7901 West 52nd Av...|       Arvada|            CO|               80002|\n",
      "|                     2|     trx-004129028377|      2022-12-09|          97|         25|    499.99|       5|    2499.95|       offer_4|          20|    1999.96|   Credit Card|   904 Walthour Road|     Savannah|            GA|               31410|\n",
      "|                     3|     trx-004528494647|      2022-08-13|          28|          6|     49.99|       3|     149.97|       offer_5|          25|    112.478|        Stripe| 10841 Sutter Circle| Sutter Creek|            CA|               95685|\n",
      "|                     4|     trx-005576570262|      2023-02-18|          96|          2|    699.99|       2|    1399.98|            NA|           0|    1399.98|        PayPal|20250 North 67th ...|     Glendale|            AZ|               85308|\n",
      "|                     5|     trx-010178568710|      2023-10-28|          82|         19|     29.99|       7|     209.93|            NA|           0|     209.93|        PayPal|      311 Oak Street|   Manchester|            CT|               06040|\n",
      "|                     6|     trx-011565841008|      2023-03-20|          99|          5|     19.99|       2|      39.98|       offer_2|          10|     35.982|        Stripe|130 Carolina Cher...|       Pooler|            GA|               31322|\n",
      "|                     7|     trx-013980447136|      2022-06-07|          60|         26|    199.99|       6|  1199.9401|       offer_2|          10|   1079.946|        Stripe|  18789 Crane Avenue|Castro Valley|            CA|               94546|\n",
      "|                     8|     trx-014939905757|      2023-09-14|          49|         21|    129.99|       1|     129.99|       offer_1|           5|    123.491|   Credit Card|9401 3rd Street Road|   Louisville|            KY|               40272|\n",
      "|                     9|     trx-015000971763|      2023-06-14|          74|          1|    999.99|       5|    4999.95|            NA|           0|    4999.95|        Stripe|8886 West 77th Ci...|       Arvada|            CO|               80005|\n",
      "|                    10|     trx-015327808327|      2022-01-24|           7|         19|     29.99|       9|     269.91|       offer_2|          10|    242.919|        Stripe|7085 West Norther...|     Glendale|            AZ|               85303|\n",
      "|                    11|     trx-018848757232|      2022-06-14|          95|         22|     79.99|       4|     319.96|            NA|           0|     319.96|   Credit Card|  1234 Carmel Street|       Madera|            CA|               93638|\n",
      "|                    12|     trx-021092172739|      2022-10-04|          99|         10|     39.99|       1|      39.99|            NA|           0|      39.99|   Credit Card|  102 Old Depot Road|   Farmington|            AR|               72730|\n",
      "|                    13|     trx-023562313683|      2023-07-28|          93|         22|     79.99|       9|     719.91|            NA|           0|     719.91|        Stripe|104 Bennington St...|     Lawrence|            MA|               01841|\n",
      "|                    14|     trx-025939079474|      2022-05-13|          33|         30|     24.99|       9|     224.91|       offer_1|           5|    213.665|        Stripe|415 West 42nd Street|     Savannah|            GA|               31401|\n",
      "|                    15|     trx-028132781255|      2022-03-24|          35|          9|    129.99|       9|    1169.91|            NA|           0|    1169.91|        Stripe|      9472 Noble Way|       Arvada|            CO|               80007|\n",
      "|                    16|     trx-029421717846|      2022-03-16|          83|         27|     29.99|      10|      299.9|            NA|           0|      299.9|        Stripe|      53 Philip Road|   Manchester|            CT|               06040|\n",
      "|                    17|     trx-031644379106|      2023-08-09|          99|         18|    149.99|       5|     749.95|       offer_4|          20|     599.96|        PayPal|323 Southeast 23r...|Oklahoma City|            OK|               73129|\n",
      "|                    18|     trx-038966531476|      2022-01-01|          50|         11|    899.99|       9|    8099.91|            NA|           0|    8099.91|        PayPal|   1783 Blakely Road|   Colchester|            VT|               05446|\n",
      "|                    19|     trx-042829887586|      2022-02-10|          70|         18|    149.99|       8|    1199.92|       offer_4|          20|    959.936|        Stripe| 788 Williamson Road|   Montgomery|            AL|               36109|\n",
      "|                    20|     trx-043758397737|      2022-09-02|          36|         26|    199.99|       9|    1799.91|       offer_3|          15|   1529.924|        Stripe|  2032 Gorgas Street|   Montgomery|            AL|               36106|\n",
      "+----------------------+---------------------+----------------+------------+-----------+----------+--------+-----------+--------------+------------+-----------+--------------+--------------------+-------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for testing purposes\n",
    "spark.sql(\"REFRESH TABLES\")\n",
    "table_dir\n",
    "test_df = spark.read.parquet(table_dir, header=True, schema=True)\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ae0e3",
   "metadata": {},
   "source": [
    "#### Offline Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2e126ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----------+--------+---------+----------+-----+----------+---------+--------------+--------------+------------+-----------+-----------+\n",
      "|transaction_date|  transaction_id|customer_id|agent_id|branch_id|product_id|units|unit_price|is_online|payment_method|offer_redeemed|discount_pct|total_price|final_price|\n",
      "+----------------+----------------+-----------+--------+---------+----------+-----+----------+---------+--------------+--------------+------------+-----------+-----------+\n",
      "|      2023-10-25|trx-072037549384|      85550|       2|        3|         3|    7|    299.99|       no|          Cash|            NA|           0|    2099.93|    2099.93|\n",
      "|      2022-05-08|trx-125208155197|      85512|       9|        5|        11|    3|    899.99|       no|   Credit Card|            NA|           0|    2699.97|    2699.97|\n",
      "+----------------+----------------+-----------+--------+---------+----------+-----+----------+---------+--------------+--------------+------------+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "offline_transactions = offline_transactions.drop(\"shipping_address\")\n",
    "offline_transactions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f492069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+------------+-----------+----------+--------+-----------+--------------+------------+-----------+--------------+----------+---------+\n",
      "|offline_transaction_id|transaction_date|customer_key|product_key|unit_price|quantity|total_price|offer_redeemed|discount_pct|final_price|payment_method|branch_key|agent_key|\n",
      "+----------------------+----------------+------------+-----------+----------+--------+-----------+--------------+------------+-----------+--------------+----------+---------+\n",
      "|      trx-072037549384|      2023-10-25|          89|          3|    299.99|       7|    2099.93|            NA|           0|    2099.93|          Cash|         3|        2|\n",
      "|      trx-125208155197|      2022-05-08|          51|         11|    899.99|       3|    2699.97|            NA|           0|    2699.97|   Credit Card|         5|        9|\n",
      "+----------------------+----------------+------------+-----------+----------+--------+-----------+--------------+------------+-----------+--------------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we need to join  the customer_dim, product_dim, branch_dim, agent_dim to the fact to get their keys\n",
    "customer_table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/customer_dim\"\n",
    "product_table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/product_dim\"\n",
    "agent_table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/agent_dim\"\n",
    "branch_table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/branch_dim\"\n",
    "\n",
    "current_customer_dim = spark.read.parquet(customer_table_dir, header=True, inferSchema=True)\n",
    "current_product_dim = spark.read.parquet(product_table_dir, header=True, inferSchema=True)\n",
    "current_agent_dim = spark.read.parquet(agent_table_dir, header=True, inferSchema=True)\n",
    "current_branch_dim = spark.read.parquet(branch_table_dir, header=True, inferSchema=True)\n",
    "\n",
    "offline_transactions_with_c_key = offline_transactions.join(current_customer_dim.select(\"customer_id\", \"customer_key\"), offline_transactions.customer_id == current_customer_dim.customer_id, \"left\")\n",
    "offline_transactions_with_c_p_key = offline_transactions_with_c_key.join(current_product_dim.select(\"product_id\", \"product_key\"), offline_transactions_with_c_key.product_id == current_product_dim.product_id, \"left\")\n",
    "offline_transactions_with_c_p_a_key = offline_transactions_with_c_p_key.join(current_agent_dim.select(\"agent_id\", \"agent_key\"), offline_transactions_with_c_p_key.agent_id == current_agent_dim.agent_id, \"left\")\n",
    "offline_transactions_with_c_p_a_b_key = offline_transactions_with_c_p_a_key.join(current_branch_dim.select(\"branch_id\", \"branch_key\"), offline_transactions_with_c_p_a_key.branch_id == current_branch_dim.branch_id, \"left\")\n",
    "\n",
    "# offline_transactions_with_c_p_a_b_key.show(2)\n",
    "columns_to_exclude = [\"customer_id\", \"product_id\", \"is_online\", \"agent_id\", \"branch_id\"]\n",
    "selected_cols = [col for col in offline_transactions_with_c_p_a_b_key.columns if col not in columns_to_exclude]\n",
    "\n",
    "final_new_offline_transactions = offline_transactions_with_c_p_a_b_key.select(selected_cols)\\\n",
    "                                                            .withColumnRenamed(\"transaction_id\", \"offline_transaction_id\")\\\n",
    "                                                            .withColumnRenamed(\"units\", \"quantity\")\n",
    "final_new_offline_transactions.columns\n",
    "desired_columns_ordered = ['offline_transaction_id', 'transaction_date', 'customer_key', 'product_key', \n",
    "                           'unit_price', 'quantity', 'total_price', 'offer_redeemed', 'discount_pct', \n",
    "                           'final_price', 'payment_method', 'branch_key', 'agent_key']\n",
    "\n",
    "# # final_new_offline_transactions.show(2)\n",
    "\n",
    "final_new_offline_transactions = final_new_offline_transactions.select(desired_columns_ordered)\n",
    "\n",
    "final_new_offline_transactions.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d112fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/itversity/q-retail-company/gold/qcompany.db/offline_transaction_fact\n"
     ]
    }
   ],
   "source": [
    "# Required Environment Variables:\n",
    "db=\"qcompany\"\n",
    "table = \"offline_transaction\"\n",
    "table_name = \"offline_transaction_fact\"\n",
    "table_key = f\"{table}_key\"\n",
    "table_id = f\"{table}_id\"\n",
    "table_dir = f\"/user/itversity/q-retail-company/gold/{db}.db/{table_name}\"\n",
    "\n",
    "print(table_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33386dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- offline_transaction_id: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- customer_key: integer (nullable = true)\n",
      " |-- product_key: integer (nullable = true)\n",
      " |-- unit_price: float (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- total_price: float (nullable = true)\n",
      " |-- offer_redeemed: string (nullable = false)\n",
      " |-- discount_pct: integer (nullable = false)\n",
      " |-- final_price: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- branch_key: integer (nullable = true)\n",
      " |-- agent_key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = final_new_offline_transactions.schema\n",
    "final_new_offline_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ad71223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty dir, let's fill it!\n",
      "----------------------------------------------------------------------------------\n",
      "adding the surrogate key\n",
      "----------------------------------------------------------------------------------\n",
      "preparing the data to be written\n",
      "----------------------------------------------------------------------------------\n",
      "writing the df as a parquet to hdfs\n",
      "----------------------------------------------------------------------------------\n",
      "\n",
      " Done\n"
     ]
    }
   ],
   "source": [
    "if is_directory_empty(table_dir):\n",
    "    print(\"Empty dir, let's fill it!\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"adding the surrogate key\")\n",
    "    df1sk = add_surrogate_key(final_new_offline_transactions, table_key, table_id)\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"preparing the data to be written\")\n",
    "    df1sk = df1sk.select(f\"{table_key}\", *[col for col in df1sk.columns if col != f\"{table_key}\"])\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing the df as a parquet to hdfs\")\n",
    "    df1sk.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Done\")\n",
    "else:\n",
    "    print(\"Dir is not empty let's Append the new transactions!\")\n",
    "    current_df_sk = spark.read.parquet(table_dir, header=True, inferSchema=True)\n",
    "    # current_df_sk.show(1)\n",
    "    print(f\"reading the data in the current {table_name} data\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"getting the max surrogate key to use later on when appening the new data... \")\n",
    "    max_sk = current_df_sk.agg(sf.max(table_key)).collect()[0][0]\n",
    "    print(f\"current max {table_key} is {max_sk}, the next {table_key} will be {max_sk+1}\")\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"writing (Appending) the new data to HDFS\")\n",
    "    final_new_offline_transactions = add_surrogate_key(final_new_offline_transactions, table_key, table_id, max_sk)\n",
    "    final_new_offline_transactions = final_new_offline_transactions.select(f\"{table_key}\", *[col for col in final_new_offline_transactions.columns if col != f\"{table_key}\"])\n",
    "    final_new_offline_transactions.write.parquet(table_dir, mode='append')\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(\"\\n Mission Passed... Respect+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47933087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing purposes\n",
    "spark.sql(\"REFRESH TABLES\")\n",
    "table_dir\n",
    "test_df = spark.read.parquet(table_dir, header=True, schema=True)\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9318e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.219055,
   "end_time": "2024-07-09T16:51:56.236044",
   "environment_variables": {},
   "exception": null,
   "input_path": "test.ipynb",
   "output_path": "test.ipynb",
   "parameters": {},
   "start_time": "2024-07-09T16:51:55.016989",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
